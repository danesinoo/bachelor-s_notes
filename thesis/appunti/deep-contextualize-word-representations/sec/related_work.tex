\section{Related Work}

Due to their ability to capture syntactic and semantic information of words from
large scale unlabeled text, pretrained word vectors are a standard component of
most state-of-the-art NLP architectures. However, these approaches for learning
word vectors only allow a single context-independent representation for each
word. Other recent work has also focused on learning context-dependent
representations.\\
In this paper, we take full advantage of access to plentiful monolingual data,
and train our biLM on a corpus with approximately 30 million sentences.\\
Previous work has also shown that different layers of deep biRNNs encode
different types of information. We show that similar signals are also induced by
the modified language model objective of our EMLo representations, and it can be
very beneficial to learn models for downstream tasks that mix these different
types of semi-supervision.\\
After pretraining the biLM with unlabeled data, we fix the weights and add
additional task-specific model capacity, allowing us to leverage large, rich and
universal biLM representations for cases where downstream training data size
dictates a smaller supervised model (note that it is the case for SST-5 and BERT
is used in the same way).
