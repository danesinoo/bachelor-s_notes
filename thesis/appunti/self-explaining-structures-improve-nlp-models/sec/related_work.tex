\section{Related Work}

In NLP, approaches to interpret naural models include extracting pieces of input
text called "rationales" as justifications to model predictions.\\
Using attentions as a tool of model interpretation, someone visualized attention
heatmaps to understand how natural language inference models build interactions
between two sentences. However, some works show the highly
inconsistency between attentions and predictors, and suggest that attentions
should not be treated as justification for a decision.\\
Saliency methods are widely used in computer vision and NLP for model
interpretation. The key idea is to find the salient features responsible for a
model's prediction.\\
In the context of NLP, somone used saliency maps to identify and extract
task-specific salient sentences from documents to maximally preserve document
topics and semantics. Someone crafted white-box adversarial examples to fine the
most salinet text-editing operations to trick models by computing derivatives.\\
The proposed framework does not require a surrogate model.
