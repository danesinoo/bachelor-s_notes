\section{Introduction}

\subsection{Kernel methods}

Here we introduce the kernel methods. By definition, kernel methods look for
linear relations in the feature space.

Basically, input items are compared via dot products of their representation in
the feature space (the most similar items have the highest dot product).
Actually, the dot product is replaced by a kernel function, which needs to be
symmetric positive and semidefinite, so that it is applied directly in the
original space (without the need to compute the feature space). So the
computational complexity is not dependent on the size of the feature space but
on the complexity of the kernel function (the generalization capability of a
kernel method depends on the number of misclassified examples in the learning
phase).

% Here it is explained something very interesting, but we will discuss it later.
Briefly, the classification of a new example is computed via weighted sum of
kernel evaluations between the example and a subset of the training instances
(note that it is very similar to the concept of algebric basis of a vector 
space). 
In this way the technique is able to directly handle structured data (if the 
kernel function is appropriate enough).\\
Kernel functions have two interesting properties:
\begin{itemize}
	\item they are closed under \textbf{addition} and \textbf{linear
		combination}. This makes different inputs easy to combine;

	\item they scale on the number of different examples, not on the number of
		features, so they do not scale on the dimensionality of the data, but on 
		the number of the data. This is very interesting if we consider 
		words as single data.
\end{itemize}

Finally, kernel function are super cool, but it is hard to find fast to compute
and expressive ones.

\subsection{Thesis motivation}

Completely expressive kernels for graphs are NP-hard to compute (so not
feasible).\\
If the data are too sparse, the kernel function is going to give sparse results
as well (it's not going to be accurate enough). So we think about a way to make
data more dense (using other kernels).\\
Another interesting thing might be keeping the information about the position of
the substructures in the trees for a better classification. Kinda making it more
convolutional and considering more the context of each node.\\
Finally, since the kernel function works on raw data, the model needs to store
those raw data, which can be a problem if the data are too big. Indeed, if the
kernel function doesn't have enough raw data to compare the new example with, it
looses accuracy; in fact, the accuracy of the classifier improves with the size
of the training set.
