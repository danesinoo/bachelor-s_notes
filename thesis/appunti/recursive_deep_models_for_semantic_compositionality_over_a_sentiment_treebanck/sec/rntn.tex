\section{Recursive Neural Tensor Network}

The main idea is to use the same tensor-based composition function for all
nodes. The composition function is defined as follows:
\begin{equation}
	p = f\left( 
	\begin{mat}{c}
		l \\
		r 
	\end{mat}^T
	V^{[1:d]}
	\begin{mat}{c}
		l \\
		r 
	\end{mat}
	+ W
	\begin{mat}{c}
		l \\
		r 
	\end{mat}
	\right)
\end{equation}

So it is introduced a new tensor $V$ that is used to compute the tensor product
of the left and right children. The tensor $W$ is defined as for the RNN. Here
follows the multiplication with the dimensions:
\begin{equation}
d = (1 \times 2d) \times (2d \times 2d \times d) \times (2d \times 1) + (d
\times 2d) \times (2d \times 1)
\end{equation}

\subsection{Tensor Backpropagation through structure}

We assume the target distribution vector at each node has a 0-1 distribution
econding. If there are $C$ classes, then it has length $C$ and a 1 at the
correct label. All other entries are 0.\\
We want to minimize the cross-entropy error between the predicted distribution
$y^i \in [0, 1]^{C \times 1}$ at node $i$ and the target distribution $t^i \in
\{0, 1\}^{C \times 1}$ at that node. This is equivalent to minimizing 
the KL-divergence between the two distributions. 

\subsubsection{Error computation}

The error is calculated as follows:
\begin{equation}
	E(\Theta) = \sum_i \sum_j t_j^i \log y_j^i + \lambda ||\Theta||^2
\end{equation}

where $\Theta = (V, W, W_s, L)$ is the set of all parameters in the model. Let's
repeat, $V$ is the nonlinear transformation of the children, $W$ is is the 
linear transformation of the children, $W_s$ is used to convert a word vector
representation to a class in the form of $y$, and $L$ is the dictionary, which
makes the word vector representation given the word.\\
Apparently, $i$ stand for the node index and $j$ for the class index.\\
Note that $t_j^i \log y_j^i$ has a non-zero value only when $t_j^i = 1$. This is
meaningful because we are only interested in the correct class. On the other
hand $\sum_j y_j = 1$ because $y$ is a probability distribution. Which means
that increasing some $j$ will decrease the others.\\

\subsubsection{Softmax Gradient}

Let $x^i \in \mathbb{R}^{d \times 1}$ be the word vector representation at 
node $i$. 
Each node backpropagates its error through to the recursively used weights $V,
W$. Let $\delta^{i, s} \in \mathbb{R}^{d \times 1}$ be the softmax error vector
at node $i$:
\begin{equation}
	\delta^{i, s} = \left( W_s^T (y^i - t^i) \right) \otimes f'(x^i)
\end{equation}

where $\otimes$ is the element-wise product and $f'$ is the elemnt-wise
derivative of $f$.

\subsubsection{Tensor Gradient}

The remaining derivatives can only be computed in a top-down fashion from the
top node through the tree and into the leaf nodes.
We define the complete incoming error for node $i$ as $\delta^{i, com}$.
The top node, only receives errors from the top node's softmax. Hence,
$\delta^{root, com} = \delta^{root, s}$.
For the derivative of each slide $k = 1, \dots, d$ we get:
\begin{equation}
	\frac{\partial E^{root}}{\partial V^{[k]}} = \delta^{root, com}_k
	\begin{mat}{c}
		l \\
		r
	\end{mat}
	\begin{mat}{c}
		l \\
		r
	\end{mat}^T
\end{equation}

where $\cdot^{[k]}$ is the $k$-th element of the vector.\\
So we can compute the error of the two children of the root node as follows:
\begin{equation}
	\delta^{root, down} = \left( W^T \delta^{root, com} + S \right) 
	\otimes f'\left(\begin{mat}{c} l \\ r \end{mat}\right)
\end{equation}

where we define
\begin{equation}
	S = \sum_{k=1}^d \delta^{root, com}_k 
	\left( V^{[k]} + \left(V^{[k]}\right)^T \right)
	\begin{mat}{c} l \\ r \end{mat}
\end{equation}

Note the similarity between $S$ and $W^T \delta^{root, com}$. $S$ can be seen as
the error that is propagated through the tensor $V$, which is $0$ in the RNNs. 
Note that both $W$ and $V$ are used to combine the children, here they are used 
to propagate the error back, indeed 
$\delta^{root, down}, S \in \mathbb{R}^{2d \times 1}$.\\
Finally 
\begin{align}
	\delta^{left, com} &= \delta^{left, s} + \delta^{root, down}[1:d] \\
	\delta^{right, com} &= \delta^{right, s} + \delta^{root, down}[d+1:2d].
\end{align}
