\section{Recursive Neural Models}

When an $n$-gram is given to the compositional models, it is parsed into a
binary tree and each leaf node corresponds to a word (my professor doens't use 
just binary trees). Recursive neural models will then cmpute parent vectors in a
bottom up fashion using different thpes of compositionality functions $g$.

\subsection{Word Vector Representations and Classification}

Each word is represented as a $d$-dimensional vector. We initialize all word
vectors by randomly sampling each value from a uniform distribution:
$\mathcal{U}(-r, r)$, where $r=0.0001$. All the word vectors are stacked in teh
word embedding matrix $L \in \mathbb{R}^{d \times |V|}$, where $|V|$ is the
vocabulary size. Note that the $L$ matrix is seen as a parameter that is learned
jointly with the compositionality models.\\

For classification into five classes, we compute the posterior probability over
labels given the word vector via:
\begin{equation}
	y^a = \text{softmax}(W_s a)
\end{equation}

where $W_s \in \mathbb{R}^{5 \times d}$ is the sentiment classification matrix,
given a word vector representation $a$. The main difference between the
models is the compositionality function $g$ used to compute the parent vector
from the child vectors $l$ and $r$.
