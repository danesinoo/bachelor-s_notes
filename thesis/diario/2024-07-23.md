---
title: "Diario"
author: "Carlo Rosso"
date: 2024-07-23
---

Ho letto il paper che descrive Star Tranformer
- [[https://paperswithcode.com/paper/star-transformer]]

Ora mi riscrivo gli appunti. Questo transformer performa in modo abbastanza
simile a bp-transformer, la differenza tra i due è il modo in cui sono valutate
le long-range dependency tra i token. Mentre star transformer dovrebbe avercele
più semplici e meno in numero, ovvero un solo nodo che ingloba e rappresenta
l'intero input; bp-transformer introduce un albero binario in cui ciascun nodo
rappresenta porzioni sempre più piccole dell'input fino ad arrivare ai token di
input. In un qualche modo bp-transformer mantiene un numero maggiore di
long-range information rispetto a ciascun token. Credo che per sfruttare
l'achittettura del bp transformer sia necessario lavorare su un dataset di
dimentsioni maggiori. Non ho fatto esperimenti in questo senso, quindi si tratta
di speculazione.

Ne ho riportato gli appunti

Ho letto il paper che usa Bert per fare fine-grained classification
- [[https://paperswithcode.com/paper/fine-grained-sentiment-classification-using]]

Ora riletto il paper e mi prendo gli appunti del caso.  
Leggendo il paper di BERT, noto che applicano un drop out di 0.1. Mentre i
kernel sono allenati con un decay factor di 0.4, o almeno i migliori hanno tale
decay factor. Quindi voglio approfondire che cosa sia il decay factor, perché da
esso credo che si riesca ad approfondire la descrizione dei kernel methods.

Finalmente, avendo steso gli appunti di BERT, mi viene in mente che ho letto un
paper che citava la rappresentazione dell'input mediante embedding. Sono
curioso, che succede se si usa una RNN sopra all'output di BERT? Semplicemente
vorrei avere la rappresentazione delle parole mediante un vettore, voglio
mantenere la struttura ad albero. 
Fondamentalmente voglio convertire il dataset usando il word embedding di BERT
per rappresentare il tutto, poi voglio ottenere lo stesso dataset e voglio
confrontare di nuovo RNN e Kernel Methods.
