---
title: "Diario"
author: "Carlo Rosso"
date: 2024-07-24
---

Ho trovato altri quattro paper i cui titoli mi paiono interessanti e quindi li
voglio studiare in questi giorni:
1. [deep contextualized word representations](https://paperswithcode.com/paper/deep-contextualized-word-representations)
1. [GPT-2](https://paperswithcode.com/paper/an-algorithm-for-routing-capsules-in-all)
1. [Self-explain](https://paperswithcode.com/paper/self-explaining-structures-improve-nlp-models)
1. [state-of-art](https://paperswithcode.com/paper/an-algorithm-for-routing-vectors-in-sequences)

Sono ordinati dal più vecchio al più recente. Li voglio leggere in ordine
cronologico perché ogni tanto si citano a vicenda e non è possibili che i paper
più vecchi citino quelli più recenti, in questo modo dovrei avere un contesto
maggiore e capire meglio i concetti.  

Ho letto questo paper [deep contextualized word representations](https://paperswithcode.com/paper/deep-contextualized-word-representations).
Ora lo voglio rileggere e prendere appunti in questo senso. Ho l'impressione che
il modello impari in modo simile a BERT, per quanto l'architettura del modello
medesimo sia più complessa, ma anche più piccola. Infatti, impara la probabilità
di una parola dato il contesto; mentre nell'allenamento di BERT viene cancellata
una parola e il modello la deve predirre.  
In ogni caso questo paper sembra introdurre una rappresentazione delle parole
nuova e più potente rispetto a quelle precedenti. Batte BERT base di poco più di
un punto percentuale, d'altra parte sopra ELMo viene messa un'architettura non
spiegata e possibilmente più complessa di quella messa sopra a BERT, per questo
motivo può darsi che la rappresentazione di BERT (anche nella sua versione base)
sia più potente di quella di ELMo (credo che sia così).  

Anzi la rappresentazione della parola mi ricorda i kernel method: "our
representations differ from traditional word type embeddings in that each token
is assigned a representation that is a function of the entire input sentence",
quindi una parola viene definita da tutte le altre parole della frase, dove i
kernel method individuano un albero sulla base di alcuni support vector, è come
se tutte le parole della frase fossero i support vector per definire quella
parola (idem per BERT).  

Riscrivendo gli appunti del paper, ho l'impressione che BERT abbia semplificato
enormente quello che si prova a fare con ELMo. La semplificazione del modello ha
una causa molto diretta: per mantenere le prestazioni c'è bisogno di una
quantità maggiore di parametri e quindi anche di dati in input. Dato il problema
che si vuole imparare, tuttavia, questo risulta essere un vantaggio, perché di
testi nella nostra lingua ne abbiamo a iosa. D'altra parte, non sarebbe bello
ottenere un modello minuscolo che riesce a fare la stessa cosa di BERT? Nel
senso che tale modello consumerebbe una quantità di energia minore davvero
gigante.
